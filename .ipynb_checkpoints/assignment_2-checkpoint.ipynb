{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "Suggested due date: 24th October 2018\n",
    "\n",
    "## Multilayer perceptron / Feedforward network\n",
    "\n",
    "The aims for this assignment are:\n",
    "* Implement a simple MLP classifier in Tensorflow\n",
    "* Train a neural network using backpropagation\n",
    "\n",
    "We will build a multilayer perceptron as a classifier, and train it using backpropagation. The MLP consists of several densely connected layers\n",
    "\n",
    "## MNIST MLP classifier\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"460\" height=\"300\" src=mnist.png>\n",
    "</p>\n",
    "\n",
    "For this assignment you will need to download the MNIST dataset, which is available <a href=“http://yann.lecun.com/exdb/mnist/“>here</a>. This dataset consists of 28x28 grayscale images, with associated labels for which digit the image contains (0-9). The training set consists of 60,000 examples and the test set is 10,000 examples.\n",
    "\n",
    "The MLP is a densely connected network, with <a href=\"https://www.codecogs.com/eqnedit.php?latex=$N$\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?$N$\" title=\"$N$\" /></a> layers <a href=\"https://www.codecogs.com/eqnedit.php?latex=$h_1,\\ldots,h_N$\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?$h_1,\\ldots,h_N$\" title=\"$h_1,\\ldots,h_N$\" /></a>, where <a href=\"https://www.codecogs.com/eqnedit.php?latex=$h_i\\in\\mathbb{R}^{n_i}$\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?$h_i\\in\\mathbb{R}^{n_i}$\" title=\"$h_i\\in\\mathbb{R}^{n_i}$\" /></a>. The input <a href=\"https://www.codecogs.com/eqnedit.php?latex=$\\mathbf{x}&space;=&space;h_1$\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?$\\mathbf{x}&space;=&space;h_1$\" title=\"$\\mathbf{x} = h_1$\" /></a> and output <a href=\"https://www.codecogs.com/eqnedit.php?latex=$\\mathbf{y}&space;=&space;h_N$\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?$\\mathbf{y}&space;=&space;h_N$\" title=\"$\\mathbf{y} = h_N$\" /></a>. For <a href=\"https://www.codecogs.com/eqnedit.php?latex=$i=1,\\ldots,N-1$\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?$i=1,\\ldots,N-1$\" title=\"$i=1,\\ldots,N-1$\" /></a>, the pre-activations are given by\n",
    "\n",
    "<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\hat{h}_{i&plus;1}&space;=&space;W^{(i)}h_i&space;&plus;&space;b^{(i)}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\hat{h}_{i&plus;1}&space;=&space;W^{(i)}h_i&space;&plus;&space;b^{(i)}\" title=\"\\hat{h}_{i+1} = W^{(i)}h_i + b^{(i)}\" /></a>,\n",
    "\n",
    "where <a href=\"https://www.codecogs.com/eqnedit.php?latex=$W^{(i)}\\in\\mathbb{R}^{n_{i&plus;1}\\times&space;n_i}$\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?$W^{(i)}\\in\\mathbb{R}^{n_{i&plus;1}\\times&space;n_i}$\" title=\"$W^{(i)}\\in\\mathbb{R}^{n_{i+1}\\times n_i}$\" /></a> and <a href=\"https://www.codecogs.com/eqnedit.php?latex=$b^{(i)}&space;\\in\\mathbb{R}^{n_{i&plus;1}}$\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?$b^{(i)}&space;\\in\\mathbb{R}^{n_{i&plus;1}}$\" title=\"$b^{(i)} \\in\\mathbb{R}^{n_{i+1}}$\" /></a>. The post-activations are given by \n",
    "\n",
    "<a href=\"https://www.codecogs.com/eqnedit.php?latex=h_{i&plus;1}&space;=&space;\\sigma&space;(\\hat{h}_{i&plus;1})\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?h_{i&plus;1}&space;=&space;\\sigma&space;(\\hat{h}_{i&plus;1})\" title=\"h_{i+1} = \\sigma (\\hat{h}_{i+1})\" /></a>,\n",
    "\n",
    "where <a href=\"https://www.codecogs.com/eqnedit.php?latex=$\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?$\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$\" title=\"$\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$\" /></a> is an activation function that is applied element-wise.\n",
    "\n",
    "For our classifier, we will flatten the inputs so it is a 784-length vector, and this will serve as input to the first hidden layer. You may also want to rescale the inputs. The output should be a 10-way softmax layer to predict the digit label.\n",
    "\n",
    "## Implementation in Tensorflow\n",
    "\n",
    "The assignment is to implement the MLP classifier for MNIST in Tensorflow, train it with one of the available optimisers and test the classification performance on the test set. Write your solution as a python script.\n",
    "\n",
    "You should choose the number of layers for the network, the size of those layers and the activation functions (try testing a few options for these hyperparameters).\n",
    "\n",
    "* Use the ```tf.layers.dense``` function for the hidden layers in the network\n",
    "* We recommend to use the ```tf.nn.sparse_softmax_cross_entropy_with_logits_v2``` to compute the loss\n",
    "* Read the TF docs carefully: the above loss function requires logits as inputs. Therefore if using this, the network output should be a linear layer\n",
    "* Create a train op in Tensorflow and train the network according to the schedule/criteria of your choice\n",
    "* Record and document the learning curves (train & test loss vs training iterations or epochs), and report the final train and test loss\n",
    "* Calculate the number of parameters used in the network, and record the time required to train the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Model\n",
    "class TorchModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28**2, 512),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = torchvision.datasets.MNIST(root=\"data/MNIST\", download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist = torch.utils.data.DataLoader(mnist_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random as rand\n",
    "plt.imshow(mnist_data[rand.randrange(len(mnist_data))][0].squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TorchModel()\n",
    "adam = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(idx):\n",
    "    one_hot = torch.zeros(idx.size(0), 10)\n",
    "    for i in range(idx.size(0)):\n",
    "        one_hot[i][idx[i].item()] = 1.\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 finished\n",
      "Batch 1 finished\n",
      "Batch 2 finished\n",
      "Batch 3 finished\n",
      "Batch 4 finished\n",
      "Batch 5 finished\n",
      "Batch 6 finished\n",
      "Batch 7 finished\n",
      "Batch 8 finished\n",
      "Batch 9 finished\n",
      "Batch 10 finished\n",
      "Batch 11 finished\n",
      "Batch 12 finished\n",
      "Batch 13 finished\n",
      "Batch 14 finished\n",
      "Batch 15 finished\n",
      "Batch 16 finished\n",
      "Batch 17 finished\n",
      "Batch 18 finished\n",
      "Batch 19 finished\n",
      "Batch 20 finished\n",
      "Batch 21 finished\n",
      "Batch 22 finished\n",
      "Batch 23 finished\n",
      "Batch 24 finished\n",
      "Batch 25 finished\n",
      "Batch 26 finished\n",
      "Batch 27 finished\n",
      "Batch 28 finished\n",
      "Batch 29 finished\n",
      "Batch 30 finished\n",
      "Batch 31 finished\n",
      "Batch 32 finished\n",
      "Batch 33 finished\n",
      "Batch 34 finished\n",
      "Batch 35 finished\n",
      "Batch 36 finished\n",
      "Batch 37 finished\n",
      "Batch 38 finished\n",
      "Batch 39 finished\n",
      "Batch 40 finished\n",
      "Batch 41 finished\n",
      "Batch 42 finished\n",
      "Batch 43 finished\n",
      "Batch 44 finished\n",
      "Batch 45 finished\n",
      "Batch 46 finished\n",
      "Batch 47 finished\n",
      "Batch 48 finished\n",
      "Batch 49 finished\n",
      "Batch 50 finished\n",
      "Batch 51 finished\n",
      "Batch 52 finished\n",
      "Batch 53 finished\n",
      "Batch 54 finished\n",
      "Batch 55 finished\n",
      "Batch 56 finished\n",
      "Batch 57 finished\n",
      "Batch 58 finished\n",
      "Batch 59 finished\n",
      "Batch 60 finished\n",
      "Batch 61 finished\n",
      "Batch 62 finished\n",
      "Batch 63 finished\n",
      "Batch 64 finished\n",
      "Batch 65 finished\n",
      "Batch 66 finished\n",
      "Batch 67 finished\n",
      "Batch 68 finished\n",
      "Batch 69 finished\n",
      "Batch 70 finished\n",
      "Batch 71 finished\n",
      "Batch 72 finished\n",
      "Batch 73 finished\n",
      "Batch 74 finished\n",
      "Batch 75 finished\n",
      "Batch 76 finished\n",
      "Batch 77 finished\n",
      "Batch 78 finished\n",
      "Batch 79 finished\n",
      "Batch 80 finished\n",
      "Batch 81 finished\n",
      "Batch 82 finished\n",
      "Batch 83 finished\n",
      "Batch 84 finished\n",
      "Batch 85 finished\n",
      "Batch 86 finished\n",
      "Batch 87 finished\n",
      "Batch 88 finished\n",
      "Batch 89 finished\n",
      "Batch 90 finished\n",
      "Batch 91 finished\n",
      "Batch 92 finished\n",
      "Batch 93 finished\n",
      "Batch 94 finished\n",
      "Batch 95 finished\n",
      "Batch 96 finished\n",
      "Batch 97 finished\n",
      "Batch 98 finished\n",
      "Batch 99 finished\n",
      "Batch 100 finished\n",
      "Batch 101 finished\n",
      "Batch 102 finished\n",
      "Batch 103 finished\n",
      "Batch 104 finished\n",
      "Batch 105 finished\n",
      "Batch 106 finished\n",
      "Batch 107 finished\n",
      "Batch 108 finished\n",
      "Batch 109 finished\n",
      "Batch 110 finished\n",
      "Batch 111 finished\n",
      "Batch 112 finished\n",
      "Batch 113 finished\n",
      "Batch 114 finished\n",
      "Batch 115 finished\n",
      "Batch 116 finished\n",
      "Batch 117 finished\n",
      "Batch 118 finished\n",
      "Batch 119 finished\n",
      "Batch 120 finished\n",
      "Batch 121 finished\n",
      "Batch 122 finished\n",
      "Batch 123 finished\n",
      "Batch 124 finished\n",
      "Batch 125 finished\n",
      "Batch 126 finished\n",
      "Batch 127 finished\n",
      "Batch 128 finished\n",
      "Batch 129 finished\n",
      "Batch 130 finished\n",
      "Batch 131 finished\n",
      "Batch 132 finished\n",
      "Batch 133 finished\n",
      "Batch 134 finished\n",
      "Batch 135 finished\n",
      "Batch 136 finished\n",
      "Batch 137 finished\n",
      "Batch 138 finished\n",
      "Batch 139 finished\n",
      "Batch 140 finished\n",
      "Batch 141 finished\n",
      "Batch 142 finished\n",
      "Batch 143 finished\n",
      "Batch 144 finished\n",
      "Batch 145 finished\n",
      "Batch 146 finished\n",
      "Batch 147 finished\n",
      "Batch 148 finished\n",
      "Batch 149 finished\n",
      "Batch 150 finished\n",
      "Batch 151 finished\n",
      "Batch 152 finished\n",
      "Batch 153 finished\n",
      "Batch 154 finished\n",
      "Batch 155 finished\n",
      "Batch 156 finished\n",
      "Batch 157 finished\n",
      "Batch 158 finished\n",
      "Batch 159 finished\n",
      "Batch 160 finished\n",
      "Batch 161 finished\n",
      "Batch 162 finished\n",
      "Batch 163 finished\n",
      "Batch 164 finished\n",
      "Batch 165 finished\n",
      "Batch 166 finished\n",
      "Batch 167 finished\n",
      "Batch 168 finished\n",
      "Batch 169 finished\n",
      "Batch 170 finished\n",
      "Batch 171 finished\n",
      "Batch 172 finished\n",
      "Batch 173 finished\n",
      "Batch 174 finished\n",
      "Batch 175 finished\n",
      "Batch 176 finished\n",
      "Batch 177 finished\n",
      "Batch 178 finished\n",
      "Batch 179 finished\n",
      "Batch 180 finished\n",
      "Batch 181 finished\n",
      "Batch 182 finished\n",
      "Batch 183 finished\n",
      "Batch 184 finished\n",
      "Batch 185 finished\n",
      "Batch 186 finished\n",
      "Batch 187 finished\n",
      "Batch 188 finished\n",
      "Batch 189 finished\n",
      "Batch 190 finished\n",
      "Batch 191 finished\n",
      "Batch 192 finished\n",
      "Batch 193 finished\n",
      "Batch 194 finished\n",
      "Batch 195 finished\n",
      "Batch 196 finished\n",
      "Batch 197 finished\n",
      "Batch 198 finished\n",
      "Batch 199 finished\n",
      "Batch 200 finished\n",
      "Batch 201 finished\n",
      "Batch 202 finished\n",
      "Batch 203 finished\n",
      "Batch 204 finished\n",
      "Batch 205 finished\n",
      "Batch 206 finished\n",
      "Batch 207 finished\n",
      "Batch 208 finished\n",
      "Batch 209 finished\n",
      "Batch 210 finished\n",
      "Batch 211 finished\n",
      "Batch 212 finished\n",
      "Batch 213 finished\n",
      "Batch 214 finished\n",
      "Batch 215 finished\n",
      "Batch 216 finished\n",
      "Batch 217 finished\n",
      "Batch 218 finished\n",
      "Batch 219 finished\n",
      "Batch 220 finished\n",
      "Batch 221 finished\n",
      "Batch 222 finished\n",
      "Batch 223 finished\n",
      "Batch 224 finished\n",
      "Batch 225 finished\n",
      "Batch 226 finished\n",
      "Batch 227 finished\n",
      "Batch 228 finished\n",
      "Batch 229 finished\n",
      "Batch 230 finished\n",
      "Batch 231 finished\n",
      "Batch 232 finished\n",
      "Batch 233 finished\n",
      "Batch 234 finished\n",
      "Batch 235 finished\n",
      "Batch 236 finished\n",
      "Batch 237 finished\n",
      "Batch 238 finished\n",
      "Batch 239 finished\n",
      "Batch 240 finished\n",
      "Batch 241 finished\n",
      "Batch 242 finished\n",
      "Batch 243 finished\n",
      "Batch 244 finished\n",
      "Batch 245 finished\n",
      "Batch 246 finished\n",
      "Batch 247 finished\n",
      "Batch 248 finished\n",
      "Batch 249 finished\n",
      "Batch 250 finished\n",
      "Batch 251 finished\n",
      "Batch 252 finished\n",
      "Batch 253 finished\n",
      "Batch 254 finished\n",
      "Batch 255 finished\n",
      "Batch 256 finished\n",
      "Batch 257 finished\n",
      "Batch 258 finished\n",
      "Batch 259 finished\n",
      "Batch 260 finished\n",
      "Batch 261 finished\n",
      "Batch 262 finished\n",
      "Batch 263 finished\n",
      "Batch 264 finished\n",
      "Batch 265 finished\n",
      "Batch 266 finished\n",
      "Batch 267 finished\n",
      "Batch 268 finished\n",
      "Batch 269 finished\n",
      "Batch 270 finished\n",
      "Batch 271 finished\n",
      "Batch 272 finished\n",
      "Batch 273 finished\n",
      "Batch 274 finished\n",
      "Batch 275 finished\n",
      "Batch 276 finished\n",
      "Batch 277 finished\n",
      "Batch 278 finished\n",
      "Batch 279 finished\n",
      "Batch 280 finished\n",
      "Batch 281 finished\n",
      "Batch 282 finished\n",
      "Batch 283 finished\n",
      "Batch 284 finished\n",
      "Batch 285 finished\n",
      "Batch 286 finished\n",
      "Batch 287 finished\n",
      "Batch 288 finished\n",
      "Batch 289 finished\n",
      "Batch 290 finished\n",
      "Batch 291 finished\n",
      "Batch 292 finished\n",
      "Batch 293 finished\n",
      "Batch 294 finished\n",
      "Batch 295 finished\n",
      "Batch 296 finished\n",
      "Batch 297 finished\n",
      "Batch 298 finished\n",
      "Batch 299 finished\n",
      "Batch 300 finished\n",
      "Batch 301 finished\n",
      "Batch 302 finished\n",
      "Batch 303 finished\n",
      "Batch 304 finished\n",
      "Batch 305 finished\n",
      "Batch 306 finished\n",
      "Batch 307 finished\n",
      "Batch 308 finished\n",
      "Batch 309 finished\n",
      "Batch 310 finished\n",
      "Batch 311 finished\n",
      "Batch 312 finished\n",
      "Batch 313 finished\n",
      "Batch 314 finished\n",
      "Batch 315 finished\n",
      "Batch 316 finished\n",
      "Batch 317 finished\n",
      "Batch 318 finished\n",
      "Batch 319 finished\n",
      "Batch 320 finished\n",
      "Batch 321 finished\n",
      "Batch 322 finished\n",
      "Batch 323 finished\n",
      "Batch 324 finished\n",
      "Batch 325 finished\n",
      "Batch 326 finished\n",
      "Batch 327 finished\n",
      "Batch 328 finished\n",
      "Batch 329 finished\n",
      "Batch 330 finished\n",
      "Batch 331 finished\n",
      "Batch 332 finished\n",
      "Batch 333 finished\n",
      "Batch 334 finished\n",
      "Batch 335 finished\n",
      "Batch 336 finished\n",
      "Batch 337 finished\n",
      "Batch 338 finished\n",
      "Batch 339 finished\n",
      "Batch 340 finished\n",
      "Batch 341 finished\n",
      "Batch 342 finished\n",
      "Batch 343 finished\n",
      "Batch 344 finished\n",
      "Batch 345 finished\n",
      "Batch 346 finished\n",
      "Batch 347 finished\n",
      "Batch 348 finished\n",
      "Batch 349 finished\n",
      "Batch 350 finished\n",
      "Batch 351 finished\n",
      "Batch 352 finished\n",
      "Batch 353 finished\n",
      "Batch 354 finished\n",
      "Batch 355 finished\n",
      "Batch 356 finished\n",
      "Batch 357 finished\n",
      "Batch 358 finished\n",
      "Batch 359 finished\n",
      "Batch 360 finished\n",
      "Batch 361 finished\n",
      "Batch 362 finished\n",
      "Batch 363 finished\n",
      "Batch 364 finished\n",
      "Batch 365 finished\n",
      "Batch 366 finished\n",
      "Batch 367 finished\n",
      "Batch 368 finished\n",
      "Batch 369 finished\n",
      "Batch 370 finished\n",
      "Batch 371 finished\n",
      "Batch 372 finished\n",
      "Batch 373 finished\n",
      "Batch 374 finished\n",
      "Batch 375 finished\n",
      "Batch 376 finished\n",
      "Batch 377 finished\n",
      "Batch 378 finished\n",
      "Batch 379 finished\n",
      "Batch 380 finished\n",
      "Batch 381 finished\n",
      "Batch 382 finished\n",
      "Batch 383 finished\n",
      "Batch 384 finished\n",
      "Batch 385 finished\n",
      "Batch 386 finished\n",
      "Batch 387 finished\n",
      "Batch 388 finished\n",
      "Batch 389 finished\n",
      "Batch 390 finished\n",
      "Batch 391 finished\n",
      "Batch 392 finished\n",
      "Batch 393 finished\n",
      "Batch 394 finished\n",
      "Batch 395 finished\n",
      "Batch 396 finished\n",
      "Batch 397 finished\n",
      "Batch 398 finished\n",
      "Batch 399 finished\n",
      "Batch 400 finished\n",
      "Batch 401 finished\n",
      "Batch 402 finished\n",
      "Batch 403 finished\n",
      "Batch 404 finished\n",
      "Batch 405 finished\n",
      "Batch 406 finished\n",
      "Batch 407 finished\n",
      "Batch 408 finished\n",
      "Batch 409 finished\n",
      "Batch 410 finished\n",
      "Batch 411 finished\n",
      "Batch 412 finished\n",
      "Batch 413 finished\n",
      "Batch 414 finished\n",
      "Batch 415 finished\n",
      "Batch 416 finished\n",
      "Batch 417 finished\n",
      "Batch 418 finished\n",
      "Batch 419 finished\n",
      "Batch 420 finished\n",
      "Batch 421 finished\n",
      "Batch 422 finished\n",
      "Batch 423 finished\n",
      "Batch 424 finished\n",
      "Batch 425 finished\n",
      "Batch 426 finished\n",
      "Batch 427 finished\n",
      "Batch 428 finished\n",
      "Batch 429 finished\n",
      "Batch 430 finished\n",
      "Batch 431 finished\n",
      "Batch 432 finished\n",
      "Batch 433 finished\n",
      "Batch 434 finished\n",
      "Batch 435 finished\n",
      "Batch 436 finished\n",
      "Batch 437 finished\n",
      "Batch 438 finished\n",
      "Batch 439 finished\n",
      "Batch 440 finished\n",
      "Batch 441 finished\n",
      "Batch 442 finished\n",
      "Batch 443 finished\n",
      "Batch 444 finished\n",
      "Batch 445 finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 446 finished\n",
      "Batch 447 finished\n",
      "Batch 448 finished\n",
      "Batch 449 finished\n",
      "Batch 450 finished\n",
      "Batch 451 finished\n",
      "Batch 452 finished\n",
      "Batch 453 finished\n",
      "Batch 454 finished\n",
      "Batch 455 finished\n",
      "Batch 456 finished\n",
      "Batch 457 finished\n",
      "Batch 458 finished\n",
      "Batch 459 finished\n",
      "Batch 460 finished\n",
      "Batch 461 finished\n",
      "Batch 462 finished\n",
      "Batch 463 finished\n",
      "Batch 464 finished\n",
      "Batch 465 finished\n",
      "Batch 466 finished\n",
      "Batch 467 finished\n",
      "Batch 468 finished\n",
      "Batch 469 finished\n",
      "Batch 470 finished\n",
      "Batch 471 finished\n",
      "Batch 472 finished\n",
      "Batch 473 finished\n",
      "Batch 474 finished\n",
      "Batch 475 finished\n",
      "Batch 476 finished\n",
      "Batch 477 finished\n",
      "Batch 478 finished\n",
      "Batch 479 finished\n",
      "Batch 480 finished\n",
      "Batch 481 finished\n",
      "Batch 482 finished\n",
      "Batch 483 finished\n",
      "Batch 484 finished\n",
      "Batch 485 finished\n",
      "Batch 486 finished\n",
      "Batch 487 finished\n",
      "Batch 488 finished\n",
      "Batch 489 finished\n",
      "Batch 490 finished\n",
      "Batch 491 finished\n",
      "Batch 492 finished\n",
      "Batch 493 finished\n",
      "Batch 494 finished\n",
      "Batch 495 finished\n",
      "Batch 496 finished\n",
      "Batch 497 finished\n",
      "Batch 498 finished\n",
      "Batch 499 finished\n",
      "Batch 500 finished\n",
      "Batch 501 finished\n",
      "Batch 502 finished\n",
      "Batch 503 finished\n",
      "Batch 504 finished\n",
      "Batch 505 finished\n",
      "Batch 506 finished\n",
      "Batch 507 finished\n",
      "Batch 508 finished\n",
      "Batch 509 finished\n",
      "Batch 510 finished\n",
      "Batch 511 finished\n",
      "Batch 512 finished\n",
      "Batch 513 finished\n",
      "Batch 514 finished\n",
      "Batch 515 finished\n",
      "Batch 516 finished\n",
      "Batch 517 finished\n",
      "Batch 518 finished\n",
      "Batch 519 finished\n",
      "Batch 520 finished\n",
      "Batch 521 finished\n",
      "Batch 522 finished\n",
      "Batch 523 finished\n",
      "Batch 524 finished\n",
      "Batch 525 finished\n",
      "Batch 526 finished\n",
      "Batch 527 finished\n",
      "Batch 528 finished\n",
      "Batch 529 finished\n",
      "Batch 530 finished\n",
      "Batch 531 finished\n",
      "Batch 532 finished\n",
      "Batch 533 finished\n",
      "Batch 534 finished\n",
      "Batch 535 finished\n",
      "Batch 536 finished\n",
      "Batch 537 finished\n",
      "Batch 538 finished\n",
      "Batch 539 finished\n",
      "Batch 540 finished\n",
      "Batch 541 finished\n",
      "Batch 542 finished\n",
      "Batch 543 finished\n",
      "Batch 544 finished\n",
      "Batch 545 finished\n",
      "Batch 546 finished\n",
      "Batch 547 finished\n",
      "Batch 548 finished\n",
      "Batch 549 finished\n",
      "Batch 550 finished\n",
      "Batch 551 finished\n",
      "Batch 552 finished\n",
      "Batch 553 finished\n",
      "Batch 554 finished\n",
      "Batch 555 finished\n",
      "Batch 556 finished\n",
      "Batch 557 finished\n",
      "Batch 558 finished\n",
      "Batch 559 finished\n",
      "Batch 560 finished\n",
      "Batch 561 finished\n",
      "Batch 562 finished\n",
      "Batch 563 finished\n",
      "Batch 564 finished\n",
      "Batch 565 finished\n",
      "Batch 566 finished\n",
      "Batch 567 finished\n",
      "Batch 568 finished\n",
      "Batch 569 finished\n",
      "Batch 570 finished\n",
      "Batch 571 finished\n",
      "Batch 572 finished\n",
      "Batch 573 finished\n",
      "Batch 574 finished\n",
      "Batch 575 finished\n",
      "Batch 576 finished\n",
      "Batch 577 finished\n",
      "Batch 578 finished\n",
      "Batch 579 finished\n",
      "Batch 580 finished\n",
      "Batch 581 finished\n",
      "Batch 582 finished\n",
      "Batch 583 finished\n",
      "Batch 584 finished\n",
      "Batch 585 finished\n",
      "Batch 586 finished\n",
      "Batch 587 finished\n",
      "Batch 588 finished\n",
      "Batch 589 finished\n",
      "Batch 590 finished\n",
      "Batch 591 finished\n",
      "Batch 592 finished\n",
      "Batch 593 finished\n",
      "Batch 594 finished\n",
      "Batch 595 finished\n",
      "Batch 596 finished\n",
      "Batch 597 finished\n",
      "Batch 598 finished\n",
      "Batch 599 finished\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(2):\n",
    "    for i, (x, y) in enumerate(mnist):\n",
    "        adam.zero_grad()\n",
    "        l = (to_one_hot(y) - model(x.view(-1, 28**2)))**2 / 2\n",
    "        l.sum().backward()\n",
    "        adam.step()\n",
    "        losses.append(l.sum() / l.size(0))\n",
    "        print(\"Batch {} finished\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(losses)), losses)\n",
    "plt.title(\"Losses\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data_test = torchvision.datasets.MNIST(root=\"data/MNIST\", train=False, download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "i = rand.randrange(len(mnist_data_test))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(mnist_data_test[i][0].squeeze())\n",
    "ax[0].set_title(\"Label: {}\".format(mnist_data_test[i][1]))\n",
    "\n",
    "model.eval()\n",
    "pred = model(mnist_data_test[i][0].view(1, 28**2)).detach().view(10)\n",
    "\n",
    "ax[1].bar(range(10), pred)\n",
    "ax[1].set_xticks(range(10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test overall performance on test-data\n",
    "test_data_loader = torch.utils.data.DataLoader(mnist_data_test, batch_size=1)\n",
    "success = 0\n",
    "fail = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for _, (x, y) in enumerate(test_data_loader):\n",
    "        if y == torch.argmax(model(x.view(-1, 28**2)).squeeze()):\n",
    "            success += 1\n",
    "        else:\n",
    "            fail += 1\n",
    "print(\"Success rate: {}%\".format(success * 100. / (success + fail)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
